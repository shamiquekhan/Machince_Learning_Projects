# Real state price prediction model
# Uses melb_data.csv in the same directory

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_absolute_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

import matplotlib.pyplot as plt

# =========================================================
# 1. Load and explore data
# =========================================================

data_path = "melb_data.csv"
melb = pd.read_csv(data_path)

print("Dataset shape:", melb.shape)
print("Columns:", melb.columns.tolist())
print("\nHead:\n", melb.head())
print("\nDescribe:\n", melb.describe(include="all"))

# Target variable (typical for Melbourne dataset)
target_col = "Price"
y = melb[target_col]

# Drop rows with missing target
melb = melb.dropna(axis=0, subset=[target_col])

# Remove target from features
X_full = melb.drop([target_col], axis=1)

# =========================================================
# 2. Basic numeric-only model (like intro + missing-values)
# =========================================================

# Use only numeric predictors
numeric_cols = [c for c in X_full.columns if X_full[c].dtype in ["int64", "float64"]]
X_num = X_full[numeric_cols].copy()

# Simple train/validation split
train_X, val_X, train_y, val_y = train_test_split(
    X_num, y, train_size=0.8, test_size=0.2, random_state=0
)

print("\nNumeric-only features:", len(numeric_cols))

# Baseline Decision Tree (simple)
from sklearn.tree import DecisionTreeRegressor

dt_model = DecisionTreeRegressor(random_state=0)
dt_model.fit(train_X, train_y)
dt_preds = dt_model.predict(val_X)
dt_mae = mean_absolute_error(val_y, dt_preds)
print("\nDecision Tree MAE (numeric only):", dt_mae)

# Random Forest on numeric-only data
rf_model_num = RandomForestRegressor(random_state=0)
rf_model_num.fit(train_X, train_y)
rf_preds = rf_model_num.predict(val_X)
rf_mae_num = mean_absolute_error(val_y, rf_preds)
print("Random Forest MAE (numeric only):", rf_mae_num)

# =========================================================
# 3. Handle missing values (SimpleImputer) on numeric data
# =========================================================

num_imputer = SimpleImputer(strategy="mean")
imputed_train_X = pd.DataFrame(num_imputer.fit_transform(train_X))
imputed_val_X = pd.DataFrame(num_imputer.transform(val_X))
imputed_train_X.columns = train_X.columns
imputed_val_X.columns = val_X.columns

rf_imputed = RandomForestRegressor(random_state=0)
rf_imputed.fit(imputed_train_X, train_y)
imputed_preds = rf_imputed.predict(imputed_val_X)
imputed_mae = mean_absolute_error(val_y, imputed_preds)
print("Random Forest MAE (numeric + imputation):", imputed_mae)

# =========================================================
# 4. Categorical handling: One-hot encoding + full pipeline
#    (like categorical-variables + pipelines lessons)
# =========================================================

# Split into train/valid again on full feature set
X_train_full, X_valid_full, y_train, y_valid = train_test_split(
    X_full, y, train_size=0.8, test_size=0.2, random_state=0
)

# Identify columns by type
cat_cols = [c for c in X_train_full.columns if X_train_full[c].dtype == "object"]
num_cols = [c for c in X_train_full.columns if X_train_full[c].dtype in ["int64", "float64"]]

print("\nNumeric columns:", len(num_cols))
print("Categorical columns:", len(cat_cols))

# Preprocessors
numeric_transformer = SimpleImputer(strategy="mean")

categorical_transformer = Pipeline(
    steps=[
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore")),
    ]
)

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_cols),
        ("cat", categorical_transformer, cat_cols),
    ]
)

# Random Forest + pipeline (like pipelines exercise)
rf_model = RandomForestRegressor(
    n_estimators=200,
    max_depth=15,
    min_samples_leaf=3,
    random_state=0,
)

rf_pipeline = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("model", rf_model),
    ]
)

rf_pipeline.fit(X_train_full, y_train)
rf_val_preds = rf_pipeline.predict(X_valid_full)
rf_val_mae = mean_absolute_error(y_valid, rf_val_preds)
print("\nRandom Forest + Pipeline MAE:", rf_val_mae)

# =========================================================
# 5. Cross-validation on pipeline (like cross-validation lesson)
# =========================================================

def get_cv_score(n_estimators: int) -> float:
    """Average MAE over 3-fold CV for RF pipeline."""
    model = RandomForestRegressor(
        n_estimators=n_estimators,
        random_state=0,
        max_depth=15,
        min
