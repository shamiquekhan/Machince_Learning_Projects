{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle House Price Prediction - Complete Notebook\n",
    "\n",
    "## Introduction to Machine Learning with Kaggle House Prices\n",
    "\n",
    "This comprehensive notebook covers the complete machine learning workflow using the Kaggle house price dataset. We'll build and compare multiple models to predict house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (13580, 21)\n",
      "\n",
      "First few rows:\n",
      "       Suburb           Address  Rooms Type      Price Method SellerG  \\\n",
      "0  Abbotsford      85 Turner St      2    h  1480000.0      S  Biggin   \n",
      "1  Abbotsford   25 Bloomburg St      2    h  1035000.0      S  Biggin   \n",
      "2  Abbotsford      5 Charles St      3    h  1465000.0     SP  Biggin   \n",
      "3  Abbotsford  40 Federation La      3    h   850000.0     PI  Biggin   \n",
      "4  Abbotsford       55a Park St      4    h  1600000.0     VB  Nelson   \n",
      "\n",
      "        Date  Distance  Postcode  ...  Bathroom  Car  Landsize  BuildingArea  \\\n",
      "0  3/12/2016       2.5    3067.0  ...       1.0  1.0     202.0           NaN   \n",
      "1  4/02/2016       2.5    3067.0  ...       1.0  0.0     156.0          79.0   \n",
      "2  4/03/2017       2.5    3067.0  ...       2.0  0.0     134.0         150.0   \n",
      "3  4/03/2017       2.5    3067.0  ...       2.0  1.0      94.0           NaN   \n",
      "4  4/06/2016       2.5    3067.0  ...       1.0  2.0     120.0         142.0   \n",
      "\n",
      "   YearBuilt  CouncilArea Lattitude  Longtitude             Regionname  \\\n",
      "0        NaN        Yarra  -37.7996    144.9984  Northern Metropolitan   \n",
      "1     1900.0        Yarra  -37.8079    144.9934  Northern Metropolitan   \n",
      "2     1900.0        Yarra  -37.8093    144.9944  Northern Metropolitan   \n",
      "3        NaN        Yarra  -37.7969    144.9969  Northern Metropolitan   \n",
      "4     2014.0        Yarra  -37.8072    144.9941  Northern Metropolitan   \n",
      "\n",
      "  Propertycount  \n",
      "0        4019.0  \n",
      "1        4019.0  \n",
      "2        4019.0  \n",
      "3        4019.0  \n",
      "4        4019.0  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "Dataset statistics:\n",
      "              Rooms         Price      Distance      Postcode      Bedroom2  \\\n",
      "count  13580.000000  1.358000e+04  13580.000000  13580.000000  13580.000000   \n",
      "mean       2.937997  1.075684e+06     10.137776   3105.301915      2.914728   \n",
      "std        0.955748  6.393107e+05      5.868725     90.676964      0.965921   \n",
      "min        1.000000  8.500000e+04      0.000000   3000.000000      0.000000   \n",
      "25%        2.000000  6.500000e+05      6.100000   3044.000000      2.000000   \n",
      "50%        3.000000  9.030000e+05      9.200000   3084.000000      3.000000   \n",
      "75%        3.000000  1.330000e+06     13.000000   3148.000000      3.000000   \n",
      "max       10.000000  9.000000e+06     48.100000   3977.000000     20.000000   \n",
      "\n",
      "           Bathroom           Car       Landsize  BuildingArea    YearBuilt  \\\n",
      "count  13580.000000  13518.000000   13580.000000   7130.000000  8205.000000   \n",
      "mean       1.534242      1.610075     558.416127    151.967650  1964.684217   \n",
      "std        0.691712      0.962634    3990.669241    541.014538    37.273762   \n",
      "min        0.000000      0.000000       0.000000      0.000000  1196.000000   \n",
      "25%        1.000000      1.000000     177.000000     93.000000  1940.000000   \n",
      "50%        1.000000      2.000000     440.000000    126.000000  1970.000000   \n",
      "75%        2.000000      2.000000     651.000000    174.000000  1999.000000   \n",
      "max        8.000000     10.000000  433014.000000  44515.000000  2018.000000   \n",
      "\n",
      "          Lattitude    Longtitude  Propertycount  \n",
      "count  13580.000000  13580.000000   13580.000000  \n",
      "mean     -37.809203    144.995216    7454.417378  \n",
      "std        0.079260      0.103916    4378.581772  \n",
      "min      -38.182550    144.431810     249.000000  \n",
      "25%      -37.856822    144.929600    4380.000000  \n",
      "50%      -37.802355    145.000100    6555.000000  \n",
      "75%      -37.756400    145.058305   10331.000000  \n",
      "max      -37.408530    145.526350   21650.000000  \n",
      "\n",
      "Column names:\n",
      "['Suburb', 'Address', 'Rooms', 'Type', 'Price', 'Method', 'SellerG', 'Date', 'Distance', 'Postcode', 'Bedroom2', 'Bathroom', 'Car', 'Landsize', 'BuildingArea', 'YearBuilt', 'CouncilArea', 'Lattitude', 'Longtitude', 'Regionname', 'Propertycount']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path of the file to read\n",
    "iowa_file_path = 'melb_data.csv'\n",
    "\n",
    "# Load the data\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", home_data.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(home_data.head())\n",
    "\n",
    "# Display statistical summary\n",
    "print(\"\\nDataset statistics:\")\n",
    "print(home_data.describe())\n",
    "\n",
    "# Display column names\n",
    "print(\"\\nColumn names:\")\n",
    "print(home_data.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'SalePrice'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\\\_libs\\\\hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SalePrice'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Define the target variable\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y \u001b[38;5;241m=\u001b[39m home_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSalePrice\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define feature columns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m feature_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLotArea\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYearBuilt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1stFlrSF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2ndFlrSF\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFullBath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBedroomAbvGr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotRmsAbvGrd\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3810\u001b[0m     ):\n\u001b[0;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'SalePrice'"
     ]
    }
   ],
   "source": [
    "# Define the target variable\n",
    "y = home_data['SalePrice']\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = ['LotArea', 'YearBuilt', '1stFlrSF', '2ndFlrSF', 'FullBath', 'BedroomAbvGr', 'TotRmsAbvGrd']\n",
    "\n",
    "# Create feature matrix X\n",
    "X = home_data[feature_columns]\n",
    "\n",
    "print(\"Features shape:\", X.shape)\n",
    "print(\"Target shape:\", y.shape)\n",
    "print(\"\\nFeatures preview:\")\n",
    "print(X.head())\n",
    "print(\"\\nTarget preview:\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split Data into Training and Validation Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data: 80% training, 20% validation\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "print(\"Training set size:\", train_X.shape[0])\n",
    "print(\"Validation set size:\", val_X.shape[0])\n",
    "print(\"Total samples:\", len(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Decision Tree Regressor (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a basic Decision Tree model\n",
    "dt_model = DecisionTreeRegressor(random_state=1)\n",
    "dt_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions\n",
    "dt_predictions = dt_model.predict(val_X)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "dt_mae = mean_absolute_error(val_y, dt_predictions)\n",
    "print(\"Decision Tree - Validation MAE: ${:,.0f}\".format(dt_mae))\n",
    "\n",
    "# Compare top predictions with actual values\n",
    "print(\"\\nTop 5 Predictions vs Actual Values:\")\n",
    "comparison_df = pd.DataFrame({'Predicted': dt_predictions[:5], 'Actual': val_y.values[:5]})\n",
    "print(comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Finding Optimal Tree Size (Hyperparameter Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different max_leaf_nodes values\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "\n",
    "# Calculate MAE for each tree size\n",
    "scores = {}\n",
    "for leaf_size in candidate_max_leaf_nodes:\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=leaf_size, random_state=1)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds)\n",
    "    scores[leaf_size] = mae\n",
    "    print(f\"max_leaf_nodes={leaf_size:4d} --> MAE: ${mae:,.0f}\")\n",
    "\n",
    "# Find the best tree size\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "print(f\"\\nBest max_leaf_nodes: {best_tree_size} with MAE: ${scores[best_tree_size]:,.0f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train Final Decision Tree with Optimal Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model using ALL data with optimal tree size\n",
    "final_dt_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n",
    "final_dt_model.fit(X, y)\n",
    "\n",
    "print(f\"Final Decision Tree model trained with max_leaf_nodes={best_tree_size}\")\n",
    "print(f\"Total samples used: {len(X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model 2: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=1)\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions\n",
    "rf_predictions = rf_model.predict(val_X)\n",
    "\n",
    "# Calculate Mean Absolute Error\n",
    "rf_mae = mean_absolute_error(val_y, rf_predictions)\n",
    "print(\"Random Forest - Validation MAE: ${:,.0f}\".format(rf_mae))\n",
    "\n",
    "# Compare top predictions\n",
    "print(\"\\nTop 5 Predictions vs Actual Values:\")\n",
    "rf_comparison_df = pd.DataFrame({'Predicted': rf_predictions[:5], 'Actual': val_y.values[:5]})\n",
    "print(rf_comparison_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Decision Tree (Basic)          MAE: ${dt_mae:,.0f}\")\n",
    "print(f\"Decision Tree (Optimized)      MAE: ${scores[best_tree_size]:,.0f}\")\n",
    "print(f\"Random Forest                  MAE: ${rf_mae:,.0f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate improvement\n",
    "improvement = ((dt_mae - rf_mae) / dt_mae) * 100\n",
    "print(f\"\\nRandom Forest improvement over baseline: {improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': feature_columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance (Random Forest):\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(feature_importance['Feature'], feature_importance['Importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Feature Importance in Random Forest Model')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Make Predictions on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Make predictions on validation set using Random Forest\n",
    "sample_predictions = rf_model.predict(val_X.head(10))\n",
    "\n",
    "print(\"Sample Predictions on Validation Data (Random Forest):\")\n",
    "sample_df = pd.DataFrame({\n",
    "    'Predicted Price': sample_predictions,\n",
    "    'Actual Price': val_y.head(10).values,\n",
    "    'Difference': sample_predictions - val_y.head(10).values,\n",
    "    'Error %': ((sample_predictions - val_y.head(10).values) / val_y.head(10).values * 100).round(2)\n",
    "})\n",
    "print(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Summary and Key Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MACHINE LEARNING PROJECT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nDataset: Kaggle House Prices\")\n",
    "print(f\"Total Samples: {len(X)}\")\n",
    "print(f\"Features Used: {len(feature_columns)}\")\n",
    "print(f\"Target Variable: SalePrice\")\n",
    "\n",
    "print(f\"\\nModels Trained:\")\n",
    "print(f\"  1. Decision Tree (Baseline)\")\n",
    "print(f\"  2. Decision Tree (Optimized with max_leaf_nodes={best_tree_size})\")\n",
    "print(f\"  3. Random Forest (100 trees)\")\n",
    "\n",
    "print(f\"\\nBest Model: Random Forest\")\n",
    "print(f\"Best Validation MAE: ${rf_mae:,.0f}\")\n",
    "\n",
    "print(f\"\\nTop 3 Most Important Features:\")\n",
    "for idx, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"  {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import math\n",
    "\n",
    "# Calculate additional metrics for Random Forest\n",
    "rmse = math.sqrt(mean_squared_error(val_y, rf_predictions))\n",
    "r2 = r2_score(val_y, rf_predictions)\n",
    "\n",
    "print(\"Random Forest Model Evaluation Metrics:\")\n",
    "print(f\"  Mean Absolute Error (MAE):      ${rf_mae:,.0f}\")\n",
    "print(f\"  Root Mean Squared Error (RMSE): ${rmse:,.0f}\")\n",
    "print(f\"  RÂ² Score:                       {r2:.4f}\")\n",
    "\n",
    "# Calculate percentage error\n",
    "percentage_error = (rf_mae / val_y.mean()) * 100\n",
    "print(f\"  Mean Percentage Error:          {percentage_error:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the complete machine learning workflow:\n",
    "\n",
    "1. **Data Loading & Exploration** - Understanding the dataset\n",
    "2. **Feature Engineering** - Selecting relevant features\n",
    "3. **Data Splitting** - Separating training and validation data\n",
    "4. **Model Building** - Creating baseline and advanced models\n",
    "5. **Hyperparameter Tuning** - Finding optimal parameters\n",
    "6. **Model Comparison** - Evaluating different approaches\n",
    "7. **Feature Analysis** - Understanding model decisions\n",
    "8. **Evaluation** - Assessing model performance\n",
    "\n",
    "The **Random Forest model** outperformed the baseline Decision Tree, demonstrating the power of ensemble methods in machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- Try other algorithms (Gradient Boosting, XGBoost)\n",
    "- Perform feature engineering and create new features\n",
    "- Handle missing values and outliers\n",
    "- Scale features and normalize data\n",
    "- Cross-validation for robust evaluation\n",
    "- Hyperparameter grid search for optimal tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
